#Program: app.py
import os
import uuid

import chainlit as cl

from langchain_core.messages import HumanMessage, AIMessage

# Import AGENTS dictionary to determine the number of contributors
from workflow import create_graph, CollaborativeState, AGENTS

# Ensure these environment variables are loaded
from dotenv import load_dotenv
load_dotenv()

# Ensure you have set your API keys
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
os.environ["GOOGLE_API_KEY"] = os.getenv("GOOGLE_API_KEY")
os.environ["ANTHROPIC_API_KEY"] = os.getenv("ANTHROPIC_API_KEY")

# --- Helper to estimate coffee break time ---
def get_coffee_break_duration():
    # Total agents minus the lead agent. Each contributor delay is 1 second (time.sleep(1))
    num_contributors = len(AGENTS) - 1
    if num_contributors < 1:
        return "1 second"
    
    # Give a rough estimate for the user
    if num_contributors == 1:
        return "2 seconds"
    return f"up to {num_contributors} seconds"

# Chainlit UI setup
@cl.on_chat_start
async def start():
    _, graph = create_graph()

    config={"configurable": {"thread_id": str(uuid.uuid4())}}

    await cl.Message(content="This panel discussion is entirely fictional. The responses are generated by AI and do not represent the actual views or opinions of the individuals mentioned.").send()

    # Initial state setup
    state = CollaborativeState(messages=[], lead_agent=[list(AGENTS.keys())[0]], human_inputs=[])
    
    # Invoking the graph initially to get the first prompt (optional, depending on graph entry point)
    # If the entry point is "human_input_received_node", we wait for the first user message.

    cl.user_session.set("graph", graph)
    cl.user_session.set("config", config)

@cl.on_message
async def on_message(message: cl.Message):
    config = cl.user_session.get("config")
    graph = cl.user_session.get("graph")
    
    # 1. Send Coffee Break Message
    duration = get_coffee_break_duration()
    coffee_message = cl.Message(
        author="System", 
        content=f"â˜• The Expert Committee is taking a quick **Coffee Break** to prepare their parallel contributions. This ensures we don't hit rate limits and might take {duration}.",
        elements=[],
        # Removed unexpected argument 'indent'
    )
    await coffee_message.send()
    
    # 2. Update state and run the graph
    graph.update_state(config, {"human_inputs": [message.content]})

    ui_messages = {}
    # Run the graph
    async for event in graph.astream_events(None, config=config, version="v2"):
        if event["event"] == "on_custom_event":
            agent_name = event["data"]["agent_name"]
            
            # Skip messages from the debate director decision logic
            if agent_name == "debate_director":
                continue 

            if event["name"] == "contributor_agent_executor":
                # Contributors run concurrently (but throttled)
                ui_message = cl.Message(author=agent_name, content=f"@{agent_name}: ", type="assistant_message")
            else:
                # Lead agent
                ui_message = cl.Message(author=agent_name, content=f"@{agent_name}: ", type="assistant_message")
            ui_messages[event["metadata"]["langgraph_checkpoint_ns"]] = ui_message
        
        # Stream the output
        if event["event"] == "on_chat_model_stream":
            ui_message = ui_messages.get(event["metadata"]["langgraph_checkpoint_ns"])
            if ui_message:
                await ui_message.stream_token(token=event["data"]["chunk"].content)
        
        # Send the final message
        if event["event"] == "on_chat_model_end":
            ui_message = ui_messages.get(event["metadata"]["langgraph_checkpoint_ns"])
            if ui_message:
                await ui_message.send()

if __name__ == "__main__":
    cl.run()